{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elie NOUHRA - TD noté matière \"SVM et ANN\" - Master 2 ECAP IAE NANTES\n",
    "Python 3.9.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD N°2 explicabilité du dataset Boston Housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Charger le dataset bostong_housing\n",
    "   1) Disponible dans le folder 04_interpretable_ml/td/data/\n",
    "   \n",
    "2) Nettoyer votre jeu de données pour créer une régression linéaire et un random forest\n",
    "   1) Tester d'ajouter des features log, quadratique, ...\n",
    "\n",
    "3)Créer un modèle baseline linéaire et random forest\n",
    "\n",
    "4) Interpréter le modèle linéaire\n",
    "\n",
    "5) Tuner votre random forest\n",
    "\n",
    "6) Interpréter globalement votre modèle meilleur modèle RF \n",
    "   1) Utiliser les PDP ou ALE & Permutation feature Importance \n",
    "   2) Comparer les résulats du random forest avec votre interprétation du modèle linéaire\n",
    "\n",
    "6) Réaliser une explicabilité par individu\n",
    "   1) En utilisant la méthode ICE (PDP individuelle)\n",
    "   2) LIME (Model local pour expliquer une prédiction)\n",
    "   3) SHAP watterfall plot (Contribution marginale de chaque variable dans la prédiction)\n",
    "\n",
    "7) Réaliser une explicabilité par individu sur le modèle RF\n",
    "- 1) ICE, le PDP est-il une bonne représentation des variables importantes de votre modèle?\n",
    "- 2) LIME\n",
    "- 3) SHAP watterfall plot\n",
    "\n",
    "8) Explorer les graphiques SHAP étudiés  dans la partie CM\n",
    "   1) beeswarm (Contribution des variables)\n",
    "   2) scatter (équivalent pdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexte du Dataset\n",
    "\n",
    "Le Boston Housing dataset est un ensemble de données couramment utilisé en apprentissage automatique et en statistique pour étudier les relations entre diverses caractéristiques socio-économiques et immobilières dans la ville de Boston.  \n",
    "Il contient des informations sur des propriétés résidentielles et leur environnement, et est souvent utilisé pour prédire la valeur des maisons, un problème classique de régression.\n",
    "\n",
    "**Variable dispo**: \n",
    "- CRIM : taux de criminalité par habitant par ville\n",
    "- ZN : proportion de terrains résidentiels zonés pour des lots de plus de 25 000 pieds carrés\n",
    "- INDUS : proportion de terrains commerciaux non commerciaux par ville\n",
    "- CHAS : variable binaire indiquant la proximité de la rivière Charles (= 1 si la zone délimitée par la ville touche la rivière ; 0 sinon)\n",
    "- NOX : concentration des oxydes d'azote (en parties par 10 millions)\n",
    "- RM : nombre moyen de pièces par logement\n",
    "- AGE : proportion des unités occupées par leur propriétaire et construites avant 1940\n",
    "- DIS : distances pondérées vers cinq centres d'emploi de Boston\n",
    "- RAD : indice d'accessibilité aux autoroutes radiales\n",
    "- TAX : taux d'imposition foncière par valeur totale pour chaque tranche de 10 000 dollars\n",
    "- PTRATIO : ratio élèves-enseignants par ville\n",
    "- LSTAT : pourcentage de la population de statut socio-économique inférieur\n",
    "- MEDV : valeur médiane des maisons occupées par leur propriétaire (en milliers de dollars) - **variable cible**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web request\n",
    "import requests\n",
    "import io\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Charger le dataset bostong_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Télécharge directement depuis Github\n",
    "url = \"https://raw.githubusercontent.com/Roulitoo/cours_iae/master/04_INTERPRETABLE_ML/td/data/boston_housing.csv\" \n",
    "download = requests.get(url).content\n",
    "\n",
    "data = pd.read_csv(io.StringIO(download.decode('utf-8')), sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aperçu des données\n",
    "data.info()\n",
    "print(data.head())\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'Unnamed' column \n",
    "data_cleaned = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# Move 'MEDV' column to the first position\n",
    "medv = data_cleaned.pop('MEDV')\n",
    "data_cleaned.insert(0, 'MEDV', medv)\n",
    "\n",
    "data_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)Nettoyer votre jeu de données pour créer une régression linéaire et un random forest\n",
    "\n",
    "Penser à :\n",
    "\n",
    "- Vérifier comment encoder vos variables qualitatives pour la modélisation \n",
    "- Analyser les distributions\n",
    "- Analyser les outliers \n",
    "- Analyser les corrélations\n",
    "\n",
    ">Tester d'ajouter des features log, quadratique, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VALEURS MANQUANTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data_cleaned.isnull().sum()\n",
    "print(\"Missing values in each column :\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DISTRIBUTION DES DONNÉES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'indice d'accessibilité aux autoroutes radiales (RAD) ne prends que 9 valeurs distinctes > passage en catégorielle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned['RAD'] = data_cleaned['RAD'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation variables catégorielles\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Analyse de la variable CHAS\n",
    "chas_counts = data_cleaned['CHAS'].value_counts()\n",
    "chas_counts.plot(kind='bar', figsize=(8, 6))\n",
    "plt.title('Bar Plot of CHAS')\n",
    "plt.xlabel('CHAS')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Nombre et pourcentage des valeurs dans CHAS\n",
    "chas_percentage = data_cleaned['CHAS'].value_counts(normalize=True) * 100\n",
    "print(\"CHAS Counts:\\n\", chas_counts)\n",
    "print(\"CHAS Percentage:\\n\", chas_percentage)\n",
    "\n",
    "# Analyse de la variable RAD\n",
    "rad_counts = data_cleaned['RAD'].value_counts()\n",
    "rad_counts.plot(kind='bar', figsize=(8, 6))\n",
    "plt.title('Bar Plot of RAD')\n",
    "plt.xlabel('RAD')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Nombre et pourcentage des valeurs dans RAD\n",
    "rad_percentage = data_cleaned['RAD'].value_counts(normalize=True) * 100\n",
    "print(\"RAD Counts:\\n\", rad_counts)\n",
    "print(\"RAD Percentage:\\n\", rad_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seulement 7% des maisons sont à proximité de la rivière Charles. L'indice d'accessibilité aux autoroutes radiales varie et la valeur 24 décroche avec les autres valeurs qui sont plus faibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Exclude categorical  columns\n",
    "columns_to_plot = data_cleaned.columns.difference(['CHAS', 'RAD'])\n",
    "\n",
    "# Boxplots\n",
    "data_cleaned[columns_to_plot].plot(kind='box', subplots=True, layout=(4, 4), figsize=(15, 15), sharex=False, sharey=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = data_cleaned[\"MEDV\"].quantile(0.25)\n",
    "Q3 = data_cleaned[\"MEDV\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "seuil_bas = Q1 - 1.5 * IQR\n",
    "seuil_haut = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = data_cleaned[(data_cleaned[\"MEDV\"] < seuil_bas) | (data_cleaned[\"MEDV\"] > seuil_haut)]\n",
    "print(outliers)\n",
    "\n",
    "print(f\"Nombre d'outliers : {outliers.shape[0]}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "sns.histplot(data_cleaned[\"MEDV\"], bins=30, kde=True, ax=axes[0], color=\"lightblue\")\n",
    "axes[0].set_title(\"Histogramme de MEDV\")\n",
    "\n",
    "sns.boxplot(y=data_cleaned[\"MEDV\"], ax=axes[1], color=\"lightblue\")\n",
    "axes[1].set_title(\"Boxplot de MEDV\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beaucoup de valeurs atypiques détectées, pour un grand nombre de variables, notamment pour la target (MEDV : la valeur médiane des maisons occupées par leur propriétaire en milliers de dollars). Nous n'allons pas modifier les variables explicatives à cette étape, nous vérifierons à l'étape de modélisation l'impact de leurs valeurs aberrantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passage de Y en log pour réduire l'influence des outliers et faciliter l'interprétation \n",
    "import numpy as np\n",
    "\n",
    "# Transformer MEDV en log\n",
    "data_cleaned[\"log_MEDV\"] = np.log(data_cleaned[\"MEDV\"])\n",
    "\n",
    "\n",
    "Q1 = data_cleaned[\"log_MEDV\"].quantile(0.25)\n",
    "Q3 = data_cleaned[\"log_MEDV\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "seuil_bas = Q1 - 1.5 * IQR\n",
    "seuil_haut = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = data_cleaned[(data_cleaned[\"log_MEDV\"] < seuil_bas) | (data_cleaned[\"log_MEDV\"] > seuil_haut)]\n",
    "print(outliers)\n",
    "\n",
    "print(f\"Nombre d'outliers : {outliers.shape[0]}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "sns.histplot(data_cleaned[\"log_MEDV\"], bins=30, kde=True, ax=axes[0], color=\"lightblue\")\n",
    "axes[0].set_title(\"Histogramme de log(MEDV)\")\n",
    "\n",
    "sns.boxplot(y=data_cleaned[\"log_MEDV\"], ax=axes[1], color=\"lightblue\")\n",
    "axes[1].set_title(\"Boxplot de log(MEDV)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de la colonne MEDV\n",
    "data_cleaned = data_cleaned.drop(columns=['MEDV'])\n",
    "\n",
    "# Vérification des colonnes restantes\n",
    "data_cleaned.info()\n",
    "\n",
    "# Move 'log_MEDV' column to the first position\n",
    "log_medv = data_cleaned.pop('log_MEDV')\n",
    "data_cleaned.insert(0, 'log_MEDV', log_medv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des statistiques descriptives\n",
    "data_cleaned.drop(columns=['CHAS', 'RAD']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interprétations : \n",
    "\n",
    "Les statistiques descriptives montrent une grande diversité entre les variables. Par exemple, **log_MEDV** (prix médian des maisons) varie de 1.61 à 3.91, avec une moyenne de 3.03, indiquant une certaine hétérogénéité des prix. En ce qui concerne **CRIM** (taux de criminalité), bien que la moyenne soit relativement modeste à 3.61, l'écart-type élevé de 8.60 reflète des disparités notables entre les zones. **ZN** (proportion de terrain résidentiel) montre également une grande variabilité, allant de 0 à 100 %, tandis que **INDUS** (proportion de terrains industriels) a une moyenne de 11.14 %, mais varie fortement entre 0.46 % et 27.74 %. **NOX** (concentration d'oxydes d'azote) est assez stable avec une moyenne de 0.55 et une faible variabilité. D'autre part, **RM** (nombre de chambres) et **AGE** (âge des maisons) varient respectivement de 3.56 à 8.78 chambres et de 2.9 à 100 ans, montrant une diversité dans les tailles et âges des maisons. **DIS** (distance aux centres d'emploi) varie de 1.13 à 12.13, reflétant des zones proches ou éloignées des centres économiques. Enfin, **TAX** (taux d'imposition) montre une forte disparité avec des valeurs allant de 187 à 711, et **LSTAT** (proportion de population à faible statut socio-économique) varie de 1.73 % à 37.97 %, soulignant des différences sociales importantes. En somme, ces variables révèlent une grande variabilité entre les zones, avec des facteurs comme la criminalité, l'âge des maisons, ou le statut socio-économique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CORRÉLATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcul des corrélations de Spearman entre toutes les variables\n",
    "correlation_matrix_spearman = data_cleaned.corr(method='spearman')\n",
    "\n",
    "# Masquer les corrélations dont la valeur absolue est inférieure à 0.6\n",
    "correlation_matrix_filtered = correlation_matrix_spearman[correlation_matrix_spearman.abs() > 0.6]\n",
    "\n",
    "# Affichage de la matrice de corrélation filtrée sous forme de heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix_filtered, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar=True, linewidths=0.5)\n",
    "plt.title(\"Matrice de Corrélation de Spearman (valeurs > 0.6 en valeur absolue)\")\n",
    "plt.show()\n",
    "\n",
    "# Afficher la matrice filtrée\n",
    "correlation_matrix_filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les corrélations de Spearman révèlent plusieurs relations clés entre les variables. Le prix médian des maisons (log_MEDV) est négativement corrélé avec la criminalité (CRIM, -0.53) et fortement lié à la taille des maisons (RM, 0.63), suggérant que des prix plus élevés sont associés à des maisons plus grandes. La criminalité est également fortement liée à la pollution (NOX, 0.82) et à l'indice industriel (INDUS, 0.74), ce qui indique que les zones avec plus de criminalité sont plus polluées et industrialisées. L'âge des maisons (AGE) est lié à une plus grande pollution (NOX, 0.80). De plus, les zones plus éloignées des centres urbains (DIS) ont moins de pollution (NOX, -0.88). La proportion de population à faible statut socio-économique (LSTAT) est fortement négative avec les prix des maisons (log_MEDV, -0.85) et corrélée à un plus petit nombre de chambres (RM, -0.64). Enfin, des taxes foncières élevées (TAX) sont associées à plus de criminalité (CRIM, 0.73) et de pollution (NOX, 0.65)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOX (concentration des oxydes d'azote) est impliqué dans 3 des 5 relations entre les variables explicatives présentant une corrélation supérieure ou égale à 0,7. Nous allons donc la supprimer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de la variable NOX\n",
    "data_cleaned = data_cleaned.drop(columns=['NOX'])\n",
    "\n",
    "# Vérification des premières lignes après la suppression\n",
    "print(data_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)Créer 2 modèles baseline, linéaire et random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les informations du DataFrame\n",
    "print(data_cleaned.info())\n",
    "\n",
    "# Sélectionner les X quantitatifs\n",
    "quantitative_columns = data_cleaned.select_dtypes(include=['float64', 'int32','int64']).columns\n",
    "quantitative_columns = quantitative_columns.difference(['RAD', 'CHAS'])\n",
    "\n",
    "# Statistiques descriptives pour les variables quantitatives\n",
    "print(\"Statistiques descriptives pour les variables quantitatives :\")\n",
    "print(data_cleaned[quantitative_columns].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for 'RAD' column\n",
    "data_encoded = pd.get_dummies(data_cleaned, columns=['RAD'], prefix='RAD', drop_first=True)\n",
    "print(data_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'RAD_24' to 'RAD_9'\n",
    "data_encoded.rename(columns={'RAD_24': 'RAD_9'}, inplace=True)\n",
    "print(data_encoded.head())\n",
    "\n",
    "# Convertir les colonnes RAD_2 à RAD_9 de booléen à int (0 ou 1)\n",
    "cols_to_convert = ['RAD_2', 'RAD_3', 'RAD_4', 'RAD_5', 'RAD_6', 'RAD_7', 'RAD_8', 'RAD_9']\n",
    "\n",
    "# Convertir en 0 et 1\n",
    "data_encoded[cols_to_convert] = data_encoded[cols_to_convert].astype(int)\n",
    "\n",
    "# Vérifier les types après la conversion\n",
    "print(data_encoded.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Interpréter le modèle linéaire\n",
    "Utiliser les méthodes intrinsèques du modèle pour l'interprétation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Définition des variables explicatives et de la variable cible\n",
    "X = data_encoded.drop(columns=['log_MEDV'])  # Variables explicatives\n",
    "y = data_encoded['log_MEDV']  # Variable dépendante\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ajouter une constante pour l'intercept\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Estimation du modèle de régression linéaire sur l'ensemble d'entraînement\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "# Résumé du modèle\n",
    "print(model.summary())\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcul du RMSE (Root Mean Squared Error)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE du modèle : {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection de variables\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "X = data_encoded.drop(columns=['log_MEDV'])  # Variables explicatives\n",
    "y = data_encoded['log_MEDV']  # Variable dépendante\n",
    "\n",
    "# Division des données en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ajouter une constante pour l'intercept\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Fonction pour effectuer la sélection de variables via backward elimination\n",
    "def backward_elimination(X_train, y_train, significance_level=0.05):\n",
    "    initial_features = X_train.columns.tolist()  # Liste des variables initiales\n",
    "    while True:\n",
    "        model = sm.OLS(y_train, X_train[initial_features]).fit()  # Estimation du modèle\n",
    "        p_values = model.pvalues[1:]  # Ignorer la constante\n",
    "        max_p_value = p_values.max()  # Prendre la plus grande p-value\n",
    "        if max_p_value > significance_level:  # Si la p-value est supérieure au seuil\n",
    "            excluded_feature = p_values.idxmax()  # Exclure la variable avec la plus grande p-value\n",
    "            initial_features.remove(excluded_feature)  # Retirer la variable de la liste\n",
    "        else:\n",
    "            break  # Arrêter si toutes les p-values sont inférieures au seuil\n",
    "    return initial_features\n",
    "\n",
    "# Appliquer la méthode de sélection de variables\n",
    "selected_features = backward_elimination(X_train, y_train)\n",
    "\n",
    "# Modèle final avec les variables sélectionnées\n",
    "model = sm.OLS(y_train, X_train[selected_features]).fit()\n",
    "\n",
    "# Résumé du modèle final\n",
    "print(model.summary())\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred = model.predict(X_test[selected_features])\n",
    "\n",
    "# Calcul du RMSE (Root Mean Squared Error)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE du modèle : {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle de régression OLS montre que plusieurs variables explicatives ont un impact significatif sur le prix médian des maisons (log_MEDV). Le R² indique que 77% de la variance de `log_MEDV` est expliquée par les variables indépendantes du modèle. Parmi les variables explicatives, `RM` (nombre moyen de chambres) a l'effet le plus positif et significatif, augmentant le prix médian des maisons de 11.94% pour chaque chambre supplémentaire. D'autres variables, telles que `CRIM` (taux de criminalité) et `LSTAT` (proportion de la population à faible statut socio-économique), ont des effets négatifs significatifs sur les prix, indiquant que des niveaux plus élevés de criminalité ou un statut socio-économique plus faible sont associés à des prix plus bas. La variable `CHAS` (proximité d'une rivière) a également un effet positif, suggérant que les maisons près d'un cours d'eau sont plus chères. De plus, les taxes foncières (`TAX`) et le ratio élèves/professeurs (`PTRATIO`) ont des effets négatifs sur les prix des maisons, tandis que la distance aux centres d'emploi (`DIS`) a également un effet négatif. Enfin, la variable `RAD_3` montre une relation positive avec le prix des maisons. Toutefois, le condition number élevé (7 760) suggère qu'il pourrait y avoir une certaine multicolinéarité entre les variables même après la sélection de variables, ce qui peut affecter la stabilité des estimations des coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Tuner votre random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Création du pipeline avec standardisation + Random Forest\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Standardisation des données\n",
    "    ('rf', RandomForestRegressor(random_state=42))  # Modèle RF sans hyperparamètres fixés\n",
    "])\n",
    "\n",
    "# Grille des hyperparamètres à tester\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [100, 200, 300],  # Nombre d'arbres\n",
    "    'rf__max_depth': [None, 10, 20],  # Profondeur maximale\n",
    "    'rf__min_samples_split': [2, 5, 10],  # Nb min d'échantillons pour une division\n",
    "    'rf__min_samples_leaf': [1, 2, 4]  # Nb min d'échantillons dans une feuille\n",
    "}\n",
    "\n",
    "# Recherche des meilleurs hyperparamètres avec validation croisée (5 folds)\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)  # Entraînement avec les données d'entraînement\n",
    "\n",
    "# Meilleurs paramètres trouvés\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Meilleurs hyperparamètres :\", best_params)\n",
    "\n",
    "# Meilleur modèle obtenu\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcul de la RMSE du meilleur modèle sur l'ensemble de test\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE du modèle Random Forest sur l'ensemble de test : {rmse}\")\n",
    "\n",
    "# Calcul de la RMSE du meilleur modèle avec validation croisée (sur les données d'entraînement)\n",
    "best_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
    "best_rmse_cv = -np.mean(best_scores)\n",
    "print(f\"RMSE moyen du meilleur modèle Random Forest (Validation croisée) : {best_rmse_cv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle Random Forest présente un bon pouvoir prédictif avec un RMSE de 0.1785 en validation croisée. Les meilleurs hyperparamètres sont : `max_depth` illimité, `min_samples_split` de 10, `min_samples_leaf` de 1, et `n_estimators` à 300. Ce modèle est plus flexible et performant que le modèle linéaire, mais son interprétation nécessite des outils comme la Permutation Feature Importance pour mieux comprendre les interactions et relations non linéaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Interpréter globalement votre meilleur modèle RF \n",
    "   1) Utiliser les PDP ou ALE & Permutation feature Importance \n",
    "   2) Comparer les résultats du random forest avec votre interprétation du modèle linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Feature Importance\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.inspection import permutation_importance\n",
    "import pandas as pd\n",
    "\n",
    "# Calcul de l'importance des variables par permutation\n",
    "perm_importance = permutation_importance(best_model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "# Visualisation de l'importance des variables\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Affichage des résultats\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `LSTAT` (le pourcentage de la population de statut socio-économique inférieur) est la plus importante, avec une contribution de plus de 50% à la prédiction de la valeur médiane des maisons. Ensuite, `RM` (le nombre moyen de pièces par logement) joue également un rôle significatif, représentant un peu plus d'1/5 de l'importance. `CRIM` (taux de criminalité) a une importance plus faible, avec une contribution d'un peu moins de 10%. Ensuite l'apport est minime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des PDP sur la variable avec la plus grande importance (LSTAT)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.inspection import permutation_importance\n",
    "import pandas as pd\n",
    "\n",
    "features_to_plot = ['LSTAT']  # Exemple de variables, à adapter\n",
    "\n",
    "# Créer un seul graphique avec un seul axe\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Affichage des PDP sur l'axe ax\n",
    "disp = PartialDependenceDisplay.from_estimator(\n",
    "    best_model, X_train, features=features_to_plot, ax=ax\n",
    ")\n",
    "\n",
    "# Ajouter un titre et afficher le graphique\n",
    "plt.title(\"Partial Dependence Plots\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La courbe PDP de `LSTAT` montre une relation décroissante entre le statut socio-économique inférieur et la valeur médiane des maisons, ce qui suggère qu'à mesure que la proportion de la population de statut inférieur augmente, la valeur des maisons tend à diminuer. Le palier à 20 indique qu'au-delà de ce seuil, l'effet de `LSTAT` sur la valeur des maisons devient négligeable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparaison des résultats du modèle random forest avec ceux du modèle linéaire : \n",
    "\n",
    "Le modèle de régression OLS met en évidence des relations linéaires et simples entre certaines variables et le prix des maisons, comme l'impact positif de `RM` et l'impact négatif de `LSTAT` et `CRIM`. Cependant, bien que ce modèle explique un peu plus des 3/4 de la variance, son interprétation peut être limitée par des problèmes de multicolinéarité, ce qui peut affecter la stabilité des estimations. En comparaison, le modèle Random Forest, avec une erreur de prédiction plus faible, offre une plus grande flexibilité et peut mieux capturer des relations non linéaires et complexes. Par exemple, la variable `LSTAT` est jugée la plus importante dans Random Forest, et la courbe PDP montre qu'au-delà d'un certain seuil de `LSTAT`, son effet devient négligeable, illustrant une dynamique plus subtile qui n'est pas capturée par le modèle linéaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Réaliser une explicabilité par individu sur le modèle RF\n",
    "- 1) ICE, le PDP est-il une bonne représentation des variables importantes de votre modèle?\n",
    "- 2) LIME\n",
    "- 3) SHAP watterfall plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import numpy as np\n",
    "\n",
    "# Sélection de la variable 'LSTAT' (par exemple)\n",
    "feature = 'LSTAT'\n",
    "\n",
    "# Créer un cadre (figure) pour afficher le graphique sans doublon\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Utiliser 'from_estimator' pour générer les plots sans créer un cadre supplémentaire\n",
    "disp = PartialDependenceDisplay.from_estimator(best_model, X_train, features=[X_train.columns.get_loc(feature)], kind=\"individual\", ax=ax)\n",
    "\n",
    "# Titre du graphique\n",
    "plt.title(f\"ICE plots pour la variable {feature}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la plupart des cas, les courbes ICE ont une trajectoire similaire, ce qui indique que l'effet observé est relativement homogène parmi les observations (augmentation de la proportion de la population de statut socio-économique inférieur tend à diminuer la valeur des maisons). Dans le cas actuel le PDP est une bonne représentation, même si les courbes ICE améliorent la compréhension des effets individuels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIME\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# Initialisation de LIME pour un modèle tabulaire\n",
    "explainer = LimeTabularExplainer(X_train.values, feature_names=X_train.columns, class_names=['target'], discretize_continuous=True)\n",
    "\n",
    "# Choix d'un individu à expliquer (par exemple le premier de l'ensemble de test)\n",
    "idx = 0  # Index de l'individu à expliquer\n",
    "exp = explainer.explain_instance(X_test.iloc[idx].values, best_model.predict, num_features=5)\n",
    "\n",
    "# Visualisation de l'explication\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erreur qui indique que LIME ne prend pas en charge les modèles de régression (comme RandomForestRegressor) dans sa version actuelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP\n",
    "import shap\n",
    "\n",
    "# Explainer SHAP basé sur l'arbre RandomForestRegressor ajusté\n",
    "explainer = shap.TreeExplainer(best_model.named_steps['rf'])\n",
    "\n",
    "# Choisir un échantillon de test à expliquer\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Afficher le SHAP waterfall plot pour un échantillon spécifique\n",
    "shap.initjs()  # Initialiser les visualisations JavaScript\n",
    "shap.waterfall_plot(shap_values[0])  # Utiliser shap_values[0] pour un exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai essayé plusieurs fois de corriger l'erreur mais sans parvenir à obtenir le graphique attendu, je suis désolé. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Explorer les graphiques SHAP étudiés  dans la partie CM\n",
    "   1) beeswarm (Contribution des variables)\n",
    "   2) scatter (équivalent pdp) - déja représenté dans la section \"6) Interpréter globalement votre modèle meilleur modèle RF \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beeswarm Plot \n",
    "import shap\n",
    "\n",
    "# Calcul des valeurs SHAP pour le modèle\n",
    "explainer = shap.TreeExplainer(best_model.named_steps['rf'])  # Utilisation du modèle Random Forest du pipeline\n",
    "shap_values = explainer.shap_values(X_train)  # Valeurs SHAP pour les données d'entraînement\n",
    "\n",
    "# Visualisation avec un beeswarm plot\n",
    "shap.summary_plot(shap_values, X_train)\n",
    "\n",
    "# Calculer la moyenne des valeurs SHAP pour chaque variable\n",
    "mean_shap_values = shap_values.mean(axis=0)\n",
    "\n",
    "# Créer un DataFrame pour stocker les résultats\n",
    "shap_summary = pd.DataFrame({\n",
    "    'Feature': X_train.columns,  # Nom de la caractéristique\n",
    "    'Mean SHAP': mean_shap_values  # Moyenne des valeurs SHAP (avec signe)\n",
    "})\n",
    "\n",
    "# Trier par ordre décroissant de l'importance SHAP (en valeur absolue)\n",
    "shap_summary = shap_summary.sort_values(by='Mean SHAP', ascending=False)\n",
    "print(shap_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variables les plus influentes sur le prix des maisons sont principalement négatives, avec un impact particulièrement fort de **LSTAT**, qui montre qu'une proportion plus élevée de la population à faible statut socio-économique diminue significativement la valeur des propriétés. Des facteurs tels que **CRIM** (criminalité) et **PTRATIO** (ratio élèves/enseignants) ont eux aussi un effet négatif sur les prix des maisons. La variables **RM** (nombre moyen de chambres) est la plus pertinente pour expliquer une variation positive du prix des maisons. Cependant, la superposition fréquente des couleurs sur le Beeswarm Plot indique que pour plusieurs facteurs la relation avec le prix des maisons n'est pas monotone. Cela suggère des relations complexes et non linéaires, mieux capturées par des modèles comme Random Forest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

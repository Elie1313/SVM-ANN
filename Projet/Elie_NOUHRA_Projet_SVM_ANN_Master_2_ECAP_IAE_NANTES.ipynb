{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elie NOUHRA - Projet matière \"SVM et ANN\" - Master 2 ECAP IAE NANTES\n",
    "Ce Jupyter Notebook contient la partie code (Python 3.9.13) et la partie Markdown. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUJET TRAITÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce dataset [Kaggle](https://www.kaggle.com/datasets/hopesb/student-depression-dataset/data) traite de la **dépression chez les étudiants en Inde** et vise à analyser, comprendre et prédire les niveaux de dépression à partir de diverses caractéristiques telles que les informations démographiques (âge, sexe), les facteurs scolaires (moyenne générale, pression ressentie), les habitudes de vie (durée de sommeil, habitudes alimentaires) et les antécédents de santé mentale (pensées suicidaires). Il comprend **27 901 observations** et **18 colonnes**, avec une **variable cible binaire**, `Depression_Status` (1 = dépression, 0 = pas de dépression). Bien que celui ayant publié le dataset n'ait pas précisé l'année et la source des données, il est probable qu'elles soient **récentes** et tirées d'**une enquête**, car la base de données a été mise à jour il y a quelques mois et semble provenir directement d'un questionnaire administré aux étudiants. Ce dataset est particulièrement pertinent pour la **recherche en santé mentale**, en offrant une **vue multidimensionnelle des facteurs contribuant à la dépression**, et pour le secteur éducatif, qui peut en tirer des enseignements pour **adapter ses mesures et soutenir le bien-être des étudiants**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHARGEMENT ET APERÇU DES DONNÉES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "import pandas\n",
    "data=pandas.read_csv('Student_Depression_Dataset.csv')\n",
    "\n",
    "# Preview of the data\n",
    "data.info()\n",
    "print(data.head())\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns to place 'Depression' in the second position\n",
    "cols = list(data.columns)\n",
    "cols.insert(1, cols.pop(cols.index('Depression')))\n",
    "data = data[cols]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VALEURS MANQUANTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "import seaborn as sns\n",
    "\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing values in each column :\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 3 observations with NA values\n",
    "data_cleaned= data.dropna()\n",
    "data_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSE EXPLORATOIRE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualisation de la répartition des valeurs de la variable cible dépression (et modifications si nécessaires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of depression\n",
    "import matplotlib.pyplot as plt\n",
    "sns.countplot(x='Depression', data=data_cleaned)\n",
    "\n",
    "plt.title('Distribution of Depression')\n",
    "plt.show()\n",
    "\n",
    "depression_counts = data_cleaned['Depression'].value_counts()\n",
    "print(depression_counts)\n",
    "\n",
    "# Calculate the proportion of depression and non-depression\n",
    "depression_proportion = data_cleaned['Depression'].value_counts(normalize=True)\n",
    "print(\"\\n\", depression_proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plus de la moitié (58.5%) des étudiants dans le dataset souffrent de dépression**.\n",
    "Je n'effectuerai pas de rééquilibrage car l'écart de représentation entre les classes est modéré, et la taille importante de l'échantillon (27 898 observations) assure une distribution suffisante des deux classes. De plus, je préfère étudier la variable cible sans modification pour éviter toute perte d'information ou introduction d'un biais artificiel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualisation de la répartion des valeurs prises par les variables explicatives catégorielles (et modifications si nécessaires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of values for object-type variables\n",
    "object_columns = data_cleaned.select_dtypes(include=['object']).columns\n",
    "\n",
    "for column in object_columns:\n",
    "    print(f\"Répartition des valeurs pour la colonne '{column}':\")\n",
    "    print(data_cleaned[column].value_counts())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les colonnes `Gender`, `Have you ever had suicidal thoughts ?` et `Family History of Mental Illness` contiennent des valeurs binaires. Il serait plus approprié de les convertir en variables de type numérique (par exemple, 1 pour \"Female\" et 0 pour \"Male\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"Gender\" to binary and rename it to \"Female\"\n",
    "data_cleaned['Female'] = data_cleaned['Gender'].apply(lambda x: 1 if x == 'Female' else 0)\n",
    "# Drop the original \"Gender\" column\n",
    "data_cleaned = data_cleaned.drop(columns=['Gender'])\n",
    "\n",
    "# Reorder columns to place 'Female' in the third position\n",
    "cols = list(data_cleaned.columns)\n",
    "cols.insert(2, cols.pop(cols.index('Female')))\n",
    "data_cleaned = data_cleaned[cols]\n",
    "\n",
    "# Convert \"Have you ever had suicidal thoughts ?\" to binary and rename it to \"Suicidal Thoughts\"\n",
    "data_cleaned['Suicidal Thoughts'] = data_cleaned['Have you ever had suicidal thoughts ?'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "# Drop the original \"Have you ever had suicidal thoughts ?\" column\n",
    "data_cleaned = data_cleaned.drop(columns=['Have you ever had suicidal thoughts ?'])\n",
    "\n",
    "# Convert \"Family History of Mental Illness\" to binary\n",
    "data_cleaned['Family History of Mental Illness'] = data_cleaned['Family History of Mental Illness'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "# Verify the changes\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passer d'un type `object` à `category` permet de réduire l'utilisation mémoire et d'optimiser les performances lors des opérations de filtrage, de regroupement ou d'agrégation, surtout lorsque la colonne contient un nombre limité de valeurs distinctes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert relevant columns to 'category' dtype\n",
    "categorical_columns = ['City', 'Profession', 'Sleep Duration', 'Dietary Habits', 'Degree']\n",
    "data_cleaned[categorical_columns] = data_cleaned[categorical_columns].astype('category')\n",
    "\n",
    "# Verify the changes\n",
    "print(data_cleaned.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of city\n",
    "sns.countplot(x='City', data=data_cleaned)\n",
    "plt.title('Distribution of City')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "city_counts = data_cleaned['City'].value_counts()\n",
    "print(city_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group cities with 2 or fewer observations\n",
    "\n",
    "# Count the occurrences of each city\n",
    "city_counts = data_cleaned['City'].value_counts()\n",
    "# Identify cities with 2 or fewer observations\n",
    "cities_to_replace = city_counts[city_counts <= 2].index\n",
    "# Replace these cities with 'Others'\n",
    "data_cleaned['City'] = data_cleaned['City'].replace(cities_to_replace, 'Others')\n",
    "# Verify the changes\n",
    "print(data_cleaned['City'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of sleep duration\n",
    "sns.countplot(x='Sleep Duration', data=data_cleaned)\n",
    "plt.title('Distribution of Sleep Duration')\n",
    "plt.show()\n",
    "\n",
    "unique_sleep_duration = data_cleaned['Sleep Duration'].unique()\n",
    "print(\"Valeurs uniques prises par 'Sleep Duration':\\n\", unique_sleep_duration)\n",
    "unique_sleep_duration = [duration.replace('Less than 5 hours', '< 5 hours').replace('More than 8 hours', '> 8 hours') for duration in unique_sleep_duration]\n",
    "print(\"Valeurs uniques prises par 'Sleep Duration':\\n\", unique_sleep_duration)\n",
    "\n",
    "sleep_duration = data_cleaned['Sleep Duration'].value_counts()\n",
    "print(sleep_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je vais exlure la `catégorie Others` de la variable `Sleep Duration` car elle semble correspondre à des réponses manquantes ou à des individus qui ne souhaitent pas répondre, ce qui ne représente pas une information pertinente pour l'analyse. De plus, le faible nombre d'observations (18) pour cette catégorie par rapport à l'ensemble des données (27 898 individus) suggère qu'elle ne contribuerait pas de manière significative à la modélisation et pourrait introduire du bruit dans les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = data_cleaned[data_cleaned['Sleep Duration'] != 'Others']\n",
    "data_cleaned['Sleep Duration'] = data_cleaned['Sleep Duration'].cat.remove_categories('Others')\n",
    "data_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of dietary habits\n",
    "sns.countplot(x='Dietary Habits', data=data_cleaned)\n",
    "plt.title('Distribution of Dietary Habits')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "dietary_habits_counts = data_cleaned['Dietary Habits'].value_counts()\n",
    "print(dietary_habits_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclure catégorie \"Others\" (Cf. justification de l'exclusion de la catégorie \"Others\" de la variable \"Sleep Duration\")\n",
    "data_cleaned = data_cleaned[data_cleaned['Dietary Habits'] != 'Others'] \n",
    "data_cleaned['Dietary Habits'] = data_cleaned['Dietary Habits'].cat.remove_categories('Others')\n",
    "data_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of profession\n",
    "sns.countplot(x='Profession', data=data_cleaned)\n",
    "plt.title('Distribution of Profession')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "profession_counts = data_cleaned['Profession'].value_counts()\n",
    "print(profession_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je vais supprimer la variable `Profession` car mon sujet porte spécifiquement sur la dépression chez les étudiants (il y a probablement eu une erreur lors de la collecte ou de l'enregistrement des données)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = data_cleaned.drop(columns=['Profession'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of degree\n",
    "sns.countplot(x='Degree', data=data_cleaned)\n",
    "plt.title('Distribution of Degree')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "degree_counts = data_cleaned['Degree'].value_counts()\n",
    "print(degree_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répartition des valeurs des variables de type category après modifications\n",
    "object_columns = data_cleaned.select_dtypes(include=['category']).columns\n",
    "\n",
    "for column in object_columns:\n",
    "    print(f\"Répartition des valeurs pour la colonne '{column}':\")\n",
    "    print(data_cleaned[column].value_counts())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualisation de la répartion des valeurs prises par les variables explicatives quantitatives (et modifications si nécessaires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les X quantitatifs\n",
    "quantitative_columns = data_cleaned.select_dtypes(include=['float64', 'int32','int64']).columns\n",
    "quantitative_columns = quantitative_columns.difference(['Depression','id'])\n",
    "\n",
    "for column in quantitative_columns:\n",
    "    print(f\"Répartition des valeurs pour la colonne '{column}':\")\n",
    "    print(data_cleaned[column].describe())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plusieurs variables lues comme float correspondent en fait à des variables binaires ou à des échelles de valeurs discrètes. Par exemple `Financial Stress` varie de 0 (pas de stress ressenti) à 5 (stress ressenti très élevé).\n",
    "La moyenne académique générale (`CPGA`) est une échelle de valeurs continues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir en integer les variables quantitatives discrètes (c'est-dire-dire toutes sauf CGPA)\n",
    "data_cleaned[quantitative_columns.difference(['CGPA'])] = data_cleaned[quantitative_columns.difference(['CGPA'])].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de la variable Job Satisfaction et Work Pressure non pertinentes pour l'analyse (Cf. justification de l'exclusion de la variable \"Profession\")\n",
    "data_cleaned.info()\n",
    "data_cleaned = data_cleaned.drop(columns=['Job Satisfaction', 'Work Pressure']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column 'Work/Study Hours' to 'Study Hours'\n",
    "data_cleaned = data_cleaned.rename(columns={'Work/Study Hours': 'Study Hours'})\n",
    "\n",
    "# Verify the changes\n",
    "print(data_cleaned.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aperçu du jeu de données après modifications (33 observations et trois variables écartées)\n",
    "data_cleaned.info()\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Détection et traitement des valeurs atypiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les X quantitatifs\n",
    "quantitative_columns = data_cleaned.select_dtypes(include=['float64', 'int32','int64']).columns\n",
    "quantitative_columns = quantitative_columns.difference(['Depression','id'])\n",
    "quantitative_columns = quantitative_columns.difference(['Family History of Mental Illness', 'Female', 'Suicidal Thoughts']) # Exclude binary variables from the quantitative columns\n",
    "\n",
    "# Calculer et afficher le nombre de valeurs distinctes pour chaque colonne quantitative\n",
    "distinct_values = data_cleaned[quantitative_columns].nunique()\n",
    "print(\"Nombre de valeurs distinctes pour les colonnes quantitatives :\")\n",
    "print(distinct_values)\n",
    "\n",
    "# Afficher les caractéristiques de base pour les  X quantitatifs\n",
    "print(data_cleaned.drop(columns=['Depression', 'id', 'Family History of Mental Illness', 'Female', 'Suicidal Thoughts']).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des distributions des X quantitatifs à l'aide de boxplots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Définir une palette de couleurs\n",
    "palette = sns.color_palette(\"husl\", len(quantitative_columns))  # Palette Husl pour des couleurs variées\n",
    "\n",
    "# Créer un boxplot esthétique pour chaque colonne quantitative\n",
    "for i, column in enumerate(quantitative_columns):\n",
    "    plt.figure(figsize=(8, 6))  # Taille de la figure\n",
    "    sns.boxplot(\n",
    "        data=data_cleaned, \n",
    "        x=column, \n",
    "        color=palette[i],  # Couleur spécifique pour chaque variable\n",
    "        width=0.6,         # Largeur du boxplot\n",
    "        saturation=0.8     # Transparence pour un rendu plus doux\n",
    "    )\n",
    "    plt.title(f'Boxplot de la variable {column}', fontsize=14, fontweight='bold', color=palette[i])\n",
    "    plt.xlabel(column, fontsize=12)\n",
    "    plt.ylabel('Valeurs', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)  # Grille discrète\n",
    "    plt.tight_layout()  # Ajustement pour éviter les chevauchements\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Valeurs atypiques** détectées pour l'`âge` (étudiants âgés) et la `moyenne académique globale` (moyenne académique nulle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for Age and CGPA\n",
    "import pandas as pd\n",
    "\n",
    "# Summary statistics for Age\n",
    "age_summary = data_cleaned['Age'].describe()\n",
    "quantiles = data_cleaned['Age'].quantile([0.80, 0.90, 0.95, 0.99])\n",
    "age_summary = pd.concat([age_summary, quantiles])\n",
    "# Replace quantile values with percentages\n",
    "age_summary.index = age_summary.index.map(lambda x: f\"{int(x * 100)}%\" if isinstance(x, float) else x)\n",
    "print(\"Summary statistics for Age with percentages:\\n\", age_summary)\n",
    "\n",
    "# Summary statistics for CGPA\n",
    "cgpa_summary = data_cleaned['CGPA'].describe()\n",
    "quantiles = data_cleaned['CGPA'].quantile([0.05,0.10,0.20,0.80, 0.90, 0.95, 0.99])\n",
    "cgpa_summary = pd.concat([cgpa_summary, quantiles])\n",
    "# Replace quantile values with percentages\n",
    "cgpa_summary.index = cgpa_summary.index.map(lambda x: f\"{int(x * 100)}%\" if isinstance(x, float) else x)\n",
    "print(\"\\nSummary statistics for CGPA with percentages:\\n\", cgpa_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction détection outliers\n",
    "def detect_outliers(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "# Detect outliers for Age\n",
    "age_outliers = detect_outliers(data_cleaned, 'Age')\n",
    "print(\"Outliers for Age:\\n\", age_outliers[['Age']])\n",
    "\n",
    "# Detect outliers for CGPA\n",
    "cgpa_outliers = detect_outliers(data_cleaned, 'CGPA')\n",
    "print(\"Outliers for CGPA:\\n\", cgpa_outliers[['CGPA']])\n",
    "\n",
    "# Exclude outliers for Age\n",
    "data_cleaned = data_cleaned[~data_cleaned.index.isin(age_outliers.index)]\n",
    "\n",
    "# Exclude outliers for CGPA\n",
    "data_cleaned = data_cleaned[~data_cleaned.index.isin(cgpa_outliers.index)]\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Data after excluding outliers:\")\n",
    "print(data_cleaned.info())\n",
    "print(data_cleaned.describe())\n",
    "\n",
    "# Number of individuals removed for Age\n",
    "removed_for_age = len(age_outliers)\n",
    "print(f\"Number of individuals removed for Age: {removed_for_age}\")\n",
    "\n",
    "# Number of individuals removed for CGPA\n",
    "removed_for_cgpa = len(cgpa_outliers)\n",
    "print(f\"Number of individuals removed for CGPA: {removed_for_cgpa}\")\n",
    "\n",
    "# Total number of individuals removed\n",
    "total_removed = len(age_outliers.index.union(cgpa_outliers.index))\n",
    "print(f\"Total number of individuals removed: {total_removed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est logique d'avoir **enlevé ces 21 étudiants aux caractéristiques incontestablement singulières** (personnes de 44 ans ou plus et individus ayant une moyenne générale nulle, peut-être car ils ne se sont jamais rendus aux cours et examens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of Age after modifications\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=data_cleaned, x='Age', color='skyblue', width=0.6, saturation=0.8)\n",
    "plt.title(\"Distribution de l'âge des étudiants sans les outliers\", fontsize=14, fontweight='bold', color='skyblue')\n",
    "plt.ylabel('Valeurs', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Boxplot of CGPA after modifications\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=data_cleaned, x='CGPA', color='lightgreen', width=0.6, saturation=0.8)\n",
    "plt.title(\"Distribution de la moyenne académique globale sans les outliers\", fontsize=14, fontweight='bold', color='lightgreen')\n",
    "plt.xlabel('CGPA', fontsize=12)\n",
    "plt.ylabel('Valeurs', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Détection et traitement des liens significatifs entre les variables explicatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre variables quantitatives continues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Sélectionner les X quantitatifs\n",
    "quantitative_columns = data_cleaned.select_dtypes(include=['float64', 'int32','int64']).columns\n",
    "quantitative_columns = quantitative_columns.difference(['Depression','id'])\n",
    "\n",
    "# Scatterplot between Age and CGPA\n",
    "sns.scatterplot(data=data_cleaned, x='Age', y='CGPA')\n",
    "plt.title('Scatterplot between Age and CGPA')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('CGPA')\n",
    "plt.show()\n",
    "\n",
    "# Calculer la corrélation de Pearson (vérifier si relation linéaire)\n",
    "pearson_corr, pearson_p_value = pearsonr(data_cleaned['Age'], data_cleaned['CGPA'])\n",
    "print(f\"Corrélation de Pearson entre l'âge et le CGPA: {pearson_corr:.4f} (p-value: {pearson_p_value:.4e})\")\n",
    "\n",
    "# Calculer la corrélation de Spearman (vérifier si relation monotone, qu'elle soit linéaire ou non)\n",
    "spearman_corr, spearman_p_value = spearmanr(data_cleaned['Age'], data_cleaned['CGPA'])\n",
    "print(f\"Corrélation de Spearman entre l'âge et le CGPA: {spearman_corr:.4f} (p-value: {spearman_p_value:.4e})\")\n",
    "\n",
    "print(\"\\nConclusion : Pas de lien significatif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre variables quantitatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n",
    "\n",
    "# Calculer la matrice de corrélation de Spearman\n",
    "correlation_matrix = data_cleaned[quantitative_columns].corr(method='spearman')\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les p-values\n",
    "p_values = pd.DataFrame(0, index=quantitative_columns, columns=quantitative_columns, dtype=float)\n",
    "\n",
    "# Calculer les p-values pour chaque paire de variables\n",
    "for col1 in quantitative_columns:\n",
    "    for col2 in quantitative_columns:\n",
    "        if col1 != col2:\n",
    "            _, p_value = spearmanr(data_cleaned[col1], data_cleaned[col2])\n",
    "            p_values.loc[col1, col2] = p_value\n",
    "\n",
    "# Filtrer les corrélations significatives (p-value < 0.05) et les coefficients > 0.25\n",
    "significant_correlations = correlation_matrix[\n",
    "    (p_values < 0.05) & (p_values != 0) & (correlation_matrix.abs() > 0.25)\n",
    "]\n",
    "\n",
    "# Afficher les corrélations significatives\n",
    "print(\"Significant Spearman correlations (p-value < 0.05 and |correlation| > 0.25):\")\n",
    "print(significant_correlations)\n",
    "\n",
    "print(\"\\nConclusion : Pas de liens significatifs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre variables quantitatives et catégorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Rename columns to remove spaces\n",
    "data_cleaned.columns = data_cleaned.columns.str.replace(' ', '_')\n",
    "\n",
    "# Update the lists of quantitative and categorical columns\n",
    "quantitative_columns = [col.replace(' ', '_') for col in quantitative_columns]\n",
    "categorical_columns = ['City', \"Sleep_Duration\", \"Dietary_Habits\", 'Degree']\n",
    "\n",
    "# Dictionnaire pour stocker les résultats de l'ANOVA\n",
    "anova_results = {}\n",
    "\n",
    "# Seuil de signification\n",
    "alpha = 0.05\n",
    "\n",
    "# Effectuer l'ANOVA pour chaque paire de variable quantitative et catégorielle\n",
    "for quant_col in quantitative_columns:\n",
    "    for cat_col in categorical_columns:\n",
    "        formula = f'{quant_col} ~ C({cat_col})'\n",
    "        model = ols(formula, data=data_cleaned).fit()\n",
    "        anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "        \n",
    "        # Vérifier si la p-value est inférieure au seuil\n",
    "        p_value = anova_table['PR(>F)'].iloc[0]  # p-value pour la première ligne (celle de la variable catégorielle)\n",
    "        if p_value < alpha:\n",
    "            anova_results[(quant_col, cat_col)] = anova_table\n",
    "\n",
    "# Afficher les résultats de l'ANOVA pour les liens significatifs\n",
    "for key, result in anova_results.items():\n",
    "    print(f'ANOVA results for {key[0]} ~ {key[1]} (p-value < {alpha}):')\n",
    "    print(result)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Les résultats de l'ANOVA montrent qu'un grand nombre de variables, telles que la pression académique et le CGPA, sont significativement liées à des facteurs comme le le diplôme, la durée de sommeil, etc.\")\n",
    "print(\"\\nPrenons l'exemple des résultats de l'ANOVA pour 'Academic_Pressure ~ City', où nous obtenons une p-value largement inférieure à 0.01 (donc bien en dessous du seuil de 5%).\")\n",
    "print(\"Cela indique qu'il existe une différence statistiquement significative dans la pression académique entre les différentes villes. En d'autres termes, le lieu (la ville) a un impact sur la pression académique.\")\n",
    "print(\"\\nCes relations peuvent entraîner des problèmes de multicolinéarité dans les estimations économétriques, car certaines variables pourraient être fortement corrélées entre elles.\")\n",
    "print(\"Pour vérifier la multicolinéarité, nous utiliserons le VIF (Variance Inflation Factor) à l'étape de modélisation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre variables catégorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Dictionnaire pour stocker les résultats du test du khi-deux\n",
    "chi2_results = {}\n",
    "\n",
    "# Définir un seuil alpha pour la significativité\n",
    "alpha = 0.05\n",
    "\n",
    "# Effectuer le test du khi-deux pour chaque paire de variables catégorielles\n",
    "for i, col1 in enumerate(categorical_columns):\n",
    "    for col2 in categorical_columns[i+1:]:  # Tester uniquement une fois chaque paire\n",
    "        contingency_table = pd.crosstab(data_cleaned[col1], data_cleaned[col2])\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "        \n",
    "        # Si la p-value est inférieure au seuil alpha, enregistrer les résultats\n",
    "        if p < alpha:\n",
    "            chi2_results[(col1, col2)] = {'chi2': chi2, 'p-value': p, 'dof': dof}\n",
    "\n",
    "# Afficher les résultats du test du khi-deux pour les liens significatifs\n",
    "if chi2_results:\n",
    "    for key, result in chi2_results.items():\n",
    "        print(f'Test du khi-deux pour {key[0]} et {key[1]} (p-value < {alpha}):')\n",
    "        print(f\"Chi2: {result['chi2']}, p-value: {result['p-value']}, dof: {result['dof']}\")\n",
    "        print('\\n')\n",
    "else:\n",
    "    print(\"Aucune relation significative trouvée entre les variables catégorielles.\")\n",
    "\n",
    "print(\"\\n Conclusion : Relations significatives relevées, cela renforce l'importance de tester la multicolinéarité.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons désormais analyser plus en profondeur les valeurs prises par les variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSE DESCRIPTIVE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.info()\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### De la dépression (variable à expliquer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the proportion of depression and non-depression\n",
    "depression_proportion = data_cleaned['Depression'].value_counts(normalize=True)\n",
    "print(\"Proportion of Depression and Non-Depression:\\n\", depression_proportion)\n",
    "\n",
    "print(\"\\nConclusion : Presque aucun changement de répartition par rapport au dataframe originel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyse descriptive des autres informations sur les étudiants (variables à expliquer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X quantitatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les X quantitatifs\n",
    "quantitative_columns = data_cleaned.select_dtypes(include=['float64', 'int32','int64']).columns\n",
    "quantitative_columns = quantitative_columns.difference(['Depression','id'])\n",
    "\n",
    "# Calculer et afficher le nombre de valeurs distinctes pour chaque colonne quantitative\n",
    "distinct_values = data_cleaned[quantitative_columns].nunique()\n",
    "print(\"\\nNombre de valeurs distinctes pour les colonnes quantitatives :\")\n",
    "print(distinct_values)\n",
    "\n",
    "# Afficher les caractéristiques de base pour les  X quantitatifs\n",
    "print(data_cleaned.drop(columns=['Depression', 'id']).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les 10 valeurs les plus fréquentes et leur fréquence relative\n",
    "for column in quantitative_columns:\n",
    "    # Obtenir les 10 premières valeurs les plus fréquentes\n",
    "    top_values = data_cleaned[column].value_counts().head(10)\n",
    "    \n",
    "    # Afficher chaque valeur et sa fréquence relative\n",
    "    print(f\"\\nLes valeurs les plus fréquentes pour {column}:\")\n",
    "    for value, count in top_values.items():\n",
    "        relative_frequency = count / len(data_cleaned)\n",
    "        print(f\" - {value}: {relative_frequency:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’échantillon est composé de **27 847 observations**, avec une répartition de **44 % de femmes et 56 % d'hommes**. L’`âge` varie de **18 à 43 ans** et la moyenne est de **26 ans**. Le **`CGPA` moyen et médian est d'approximativement 7.7** (min : 5, max : 10), avec des valeurs élevées relativement fréquentes. Les étudiants consacrent en moyenne **7h10** par jour aux études, et plus de **50 % étudient entre 6 et 10 heures**. La **`satisfaction aux études` est assez hétérogène**, avec les niveaux 1, 2, 3 et 4 représentant environ 20 % chacun.  \n",
    "\n",
    "La **`pression académique` moyenne est de 3.1 sur 5, mais 41 % des étudiants s'identifient aux deux intensités les plus fortes**. Le **`stress financier ressenti` est similaire** (moyenne de 3.1 sur 5 avec 45 % d'étudiants qui déclarent un stress au plus haut niveau ou juste en dessous). Environ **48 % ont des `antécédents familiaux de troubles mentaux`**, et **63 % déclarent avoir eu des `pensées suicidaires`**, une statistique préoccupante.  \n",
    "\n",
    "Ces éléments mettent en lumière le fait que le **panel d'individus interrogés semble assez anxieux et fragile**. On peut **suspecter que ces caractéristiques se reflètent dans la dépression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Create boxplots for 'Age' and 'CGPA'\n",
    "for column in ['Age', 'CGPA']:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(\n",
    "        data=data_cleaned, \n",
    "        x=column, \n",
    "        color=palette[quantitative_columns.get_loc(column)],  # Use the same color palette\n",
    "        width=0.6, \n",
    "        saturation=0.8\n",
    "    )\n",
    "    plt.title(f'Boxplot de la variable {column}', fontsize=14, fontweight='bold', color=palette[quantitative_columns.get_loc(column)])\n",
    "    plt.xlabel(column, fontsize=12)\n",
    "    plt.ylabel('Valeurs', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "## Créer des barplots pour chaque colonne sélectionnée\n",
    "\n",
    "# Liste des colonnes à exclure\n",
    "exclude_columns = ['Age', 'CGPA', 'Family_History_of_Mental_Illness', 'Female', 'Suicidal_Thoughts' ]\n",
    "# Sélectionner les colonnes à inclure\n",
    "quantitative_columns_to_plot = data_cleaned.select_dtypes(include=['int32', 'float64']).columns.difference(exclude_columns)\n",
    "\n",
    "for column in quantitative_columns_to_plot:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Calculer la fréquence relative\n",
    "    relative_freq = data_cleaned[column].value_counts(normalize=True)\n",
    "    sns.barplot(x=relative_freq.index, y=relative_freq.values, palette='husl')\n",
    "    plt.title(f'Fréquence relative de la variable {column}', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(column, fontsize=12)\n",
    "    plt.ylabel('Fréquence relative', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos précédents commentaires se reflètent dans ces graphiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X qualitatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les caractéristiques de base pour les colonnes catégorielles\n",
    "categorical_columns = ['City', 'Sleep_Duration', 'Dietary_Habits', 'Degree']\n",
    "data_cleaned[categorical_columns] = data_cleaned[categorical_columns].astype('category')\n",
    "print(data_cleaned.info())\n",
    "print(data_cleaned.describe(include=['category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les 10 valeurs les plus fréquentes et leur fréquence relative pour chaque variable catégorielle\n",
    "for column in categorical_columns:\n",
    "    # Obtenir les 10 premières valeurs les plus fréquentes\n",
    "    top_values = data_cleaned[column].value_counts().head(10)\n",
    "    \n",
    "    # Afficher chaque valeur et sa fréquence relative\n",
    "    print(f\"\\nLes valeurs les plus fréquentes pour {column}:\")\n",
    "    for value, count in top_values.items():\n",
    "        relative_frequency = count / len(data_cleaned)\n",
    "        print(f\" - {value}: {relative_frequency:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'échantillon couvre **31 `villes` différentes**, avec Kalyan en tête (5,6 % des réponses), suivie de Srinagar (4,9 %) et Hyderabad (4,8 %), suggérant une **représentation géographique relativement large**, bien que certaines villes apparaissent un peu plus fréquemment. **La `durée du sommeil` est fortement hétérogène**. En effet, bien que **30 % des étudiants dorment moins de 5 heures par nuit**, les autres groupes sont également bien représentés (entre 22 et 27 %). Pour les **`habitudes alimentaires`, une grande majorité des étudiants ont des habitudes considérées comme non optimales**, avec environ 37 % ayant des habitudes \"non saines\", suivis de ceux ayant des habitudes \"modérées\" (35,6 %). Seuls 27,4 % des étudiants adoptent un régime alimentaire \"sain\". Ces chiffres peuvent être liés à l'anxiété et au bien-être plus généralement. Enfin, concernant les **28 `diplômes`**, **plus d'un cinquième des répondants sont en Classe 12** (**21,8 %**), suivis par ceux en B.Ed (6,7 %) et B.Com (5,4 %). Les trois cursus évoqués correspondent respectivement à la dernière année du secondaire, un diplôme de premier cycle formant les enseignants et une licence en commerce et gestion. L'ensemble de ces éléments permet de mieux comprendre les caractéristiques de l'échantillon et soulève à nouveau des **tendances préoccupantes pour une part non négligeable des étudiants**, notamment en ce qui concerne la durée du sommeil et les habitudes alimentaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces observations permettent de mieux comprendre la situation mentale et le cadre de vie des étudiants de l'échantillon. \n",
    "Nous pouvons à présent passer à l'estimation de modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODÉLISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les informations du DataFrame\n",
    "print(data_cleaned.info())\n",
    "\n",
    "# Statistiques descriptives pour les variables quantitatives\n",
    "print(\"Statistiques descriptives pour les variables quantitatives :\")\n",
    "print(data_cleaned[quantitative_columns].describe())\n",
    "\n",
    "# Fréquences pour les variables catégorielles\n",
    "print(\"\\nFréquences pour les variables catégorielles :\")\n",
    "for column in categorical_columns:\n",
    "    print(f\"\\nValeurs pour la colonne '{column}':\")\n",
    "    print(data_cleaned[column].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la modélisation je vais m'appuyer sur **deux méthodes**, la `régression logistique avec le package statsmodels` et le `Gradient Boosting avec la bibliothèque XGBoost`. J'ai choisi la première car elle est parfaitement adaptée à un problème de classification binaire et est aisément interprétable. En complément, XGBoost est un modèle plus complexe et puissant qui peut capturer des relations non linéaires et gérer des interactions complexes entre les variables, offrant ainsi une meilleure performance dans des contextes de grande dimension et de données complexes. **Ensemble, ces deux modèles permettent de comparer la précision et l'interprétabilité d'approches simples et avancées**. \n",
    "Après avoir préparé les données, nous estimerons les modèles, les comparerons à l'aide de divers indicateurs de performance puis nous en interpréterons les résultats en essayant d'identifier dans quelle mesure certains facteurs sont liés à la dépression des étudiants interrogés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage one-hot des variables catégorielles\n",
    "data_encoded = pd.get_dummies(data_cleaned, columns=['Dietary_Habits', 'Sleep_Duration','Degree', 'City'], drop_first=True)\n",
    "\n",
    "# Verify the changes\n",
    "print(data_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que la `régression logistique` puisse bénéficier de la standardisation, dans mon cas, **les variables n'ont pas de différences d'échelle extrêmes**. En outre garder les valeurs telles quelles permet de **faciliter l'interprétation**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the features (X) and the target (y)\n",
    "X = data_encoded.drop(columns=['Depression', 'id'])\n",
    "y = data_encoded['Depression']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the shapes of the resulting datasets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Vérification des informations des DataFrames \n",
    "print(data_encoded.info())\n",
    "print(X_train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Régression logistique binaire (bibliothèque statsmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "\n",
    "# Convert boolean columns to integers\n",
    "X_train = X_train.astype(int)\n",
    "\n",
    "# Ajouter une constante pour l'interception\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "\n",
    "# Créer le modèle de régression logistique\n",
    "logit_model = sm.Logit(y_train, X_train_const)\n",
    "\n",
    "# Ajuster le modèle\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Prédire les valeurs pour l'ensemble de test\n",
    "X_test_const = sm.add_constant(X_test)\n",
    "\n",
    "# Ensure there are no invalid values in X_test_const\n",
    "X_test_const = X_test_const.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification de la multicolinéarité à l'aide du `VIF` (`Variance Inflation Factor`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_train_const.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_train_const.values, i) for i in range(X_train_const.shape[1])]\n",
    "\n",
    "# Trier le DataFrame par VIF de manière décroissante\n",
    "vif_data_sorted = vif_data.sort_values(by=\"VIF\", ascending=False)\n",
    "\n",
    "# Changer la configuration d'affichage pour afficher toutes les lignes\n",
    "pd.set_option('display.max_rows', None)  # Afficher toutes les lignes\n",
    "pd.set_option('display.max_columns', None)  # Afficher toutes les colonnes\n",
    "\n",
    "# Afficher le DataFrame trié\n",
    "print(vif_data_sorted)\n",
    "\n",
    "# Réinitialiser les options pour revenir à la configuration par défaut\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les villes posent clairement problème. Nous allons les regrouper selon leur taille (population) en nous appuyant sur la source officielle pour les données démographiques en Inde, à savoir [Census of India](https://censusindia.gov.in/census.website/). Nous avons choisi de les séparer en trois catégories : petites villes (moins de 1 million d'habitants), villes moyennes (entre 1 et 5 millions d'habitants) et grandes villes (plus de 5 millions d'habitants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map cities to their sizes\n",
    "city_size_mapping = {\n",
    "    \"Faridabad\": \"Small\",\n",
    "    \"Ghaziabad\": \"Small\",\n",
    "    \"Rajkot\": \"Small\",\n",
    "    \"Indore\": \"Small\",\n",
    "    \"Kanpur\": \"Small\",\n",
    "    \"Meerut\": \"Small\",\n",
    "    \"Varanasi\": \"Small\",\n",
    "    \"Nashik\": \"Small\",\n",
    "    \"Nagpur\": \"Small\",\n",
    "    \"Ahmedabad\": \"Medium\",\n",
    "    \"Pune\": \"Medium\",\n",
    "    \"Surat\": \"Medium\",\n",
    "    \"Jaipur\": \"Medium\",\n",
    "    \"Lucknow\": \"Medium\",\n",
    "    \"Vadodara\": \"Medium\",\n",
    "    \"Bhopal\": \"Medium\",\n",
    "    \"Patna\": \"Medium\",\n",
    "    \"Ludhiana\": \"Medium\",\n",
    "    \"Srinagar\": \"Medium\",\n",
    "    \"Visakhapatnam\": \"Medium\",\n",
    "    \"Kalyan\": \"Medium\",\n",
    "    \"Thane\": \"Medium\",\n",
    "    \"Mumbai\": \"Large\",\n",
    "    \"Delhi\": \"Large\",\n",
    "    \"Bangalore\": \"Large\",\n",
    "    \"Hyderabad\": \"Large\",\n",
    "    \"Chennai\": \"Large\",\n",
    "    \"Kolkata\": \"Large\"\n",
    "}\n",
    "\n",
    "# Map cities to their sizes\n",
    "data_cleaned['City_Size'] = data_cleaned['City'].map(city_size_mapping)\n",
    "\n",
    "# Verify the changes\n",
    "print(data_cleaned[['City', 'City_Size']].head())\n",
    "\n",
    "# Encodage one-hot des variables catégorielles\n",
    "data_encoded = pd.get_dummies(data_cleaned, columns=['Dietary_Habits', 'Sleep_Duration','Degree', 'City_Size'], drop_first=True)\n",
    "\n",
    "# Remove the 'City' variable from data_encoded\n",
    "data_encoded = data_encoded.drop(columns=['City'])\n",
    "\n",
    "# Verify the changes\n",
    "print(data_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the features (X) and the target (y)\n",
    "X = data_encoded.drop(columns=['Depression', 'id'])\n",
    "y = data_encoded['Depression']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the shapes of the resulting datasets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Vérification des informations des DataFrames \n",
    "print(data_encoded.info())\n",
    "print(X_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réestimation du modèle de régression logistique\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Convert boolean columns to integers\n",
    "X_train = X_train.astype(int)\n",
    "\n",
    "# Ajouter une constante pour l'interception\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "\n",
    "# Créer le modèle de régression logistique\n",
    "logit_model = sm.Logit(y_train, X_train_const)\n",
    "\n",
    "# Ajuster le modèle\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Prédire les valeurs pour l'ensemble de test\n",
    "X_test_const = sm.add_constant(X_test)\n",
    "\n",
    "# Ensure there are no invalid values in X_test_const\n",
    "X_test_const = X_test_const.astype(float)\n",
    "\n",
    "y_pred = result.predict(X_test_const)\n",
    "y_pred_class = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_train_const.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_train_const.values, i) for i in range(X_train_const.shape[1])]\n",
    "\n",
    "# Trier le DataFrame par VIF de manière décroissante\n",
    "vif_data_sorted = vif_data.sort_values(by=\"VIF\", ascending=False)\n",
    "\n",
    "# Changer la configuration d'affichage pour afficher toutes les lignes\n",
    "pd.set_option('display.max_rows', None)  # Afficher toutes les lignes\n",
    "pd.set_option('display.max_columns', None)  # Afficher toutes les colonnes\n",
    "\n",
    "# Afficher le DataFrame trié\n",
    "print(vif_data_sorted)\n",
    "\n",
    "# Réinitialiser les options pour revenir à la configuration par défaut\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les `VIF` des variables sont à présent **inférieurs à 5, ce qui indique une faible multicolinéarité entre les facteurs**, garantissant des estimations plus stables et interprétables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicateurs de performance du modèle \n",
    "\n",
    "y_pred = result.predict(X_test_const)\n",
    "y_pred_class = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "# Matrice de confusion\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_class)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Précision\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Précision (Precision)\n",
    "precision = precision_score(y_test, y_pred_class)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Rappel (Recall)\n",
    "recall = recall_score(y_test, y_pred_class)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Score F1\n",
    "f1 = f1_score(y_test, y_pred_class)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# AUC-ROC\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"AUC-ROC:\", roc_auc)\n",
    "\n",
    "print(\"\\nNous comparerons ces résultats avec ceux du modèle XGBoost.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting (bibliothèque XGboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimer un modèle XGBoost avec validation croisée (CV) plutôt qu'avec les paramètres par défaut permet, en théorie, d'obtenir une évaluation plus robuste de sa performance et d'optimiser ses hyperparamètres. C'est ce que nous allons vérifier. \n",
    "NB : Il n'est pas nécessaire de vérifier la multicolinéarité pour XGBoost car il s'agit d'un modèle basé sur des arbres de décision, qui sélectionnent les variables de manière séquentielle et ne sont pas affectés par la colinéarité entre les prédicteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle par défaut\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the XGBoost model with default parameters\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the values for the test set\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb)\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb)\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb)\n",
    "roc_auc_xgb = roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "\n",
    "print(\"XGBoost Model Performance:\")\n",
    "print(\"Accuracy:\", accuracy_xgb)\n",
    "print(\"Precision:\", precision_xgb)\n",
    "print(\"Recall:\", recall_xgb)\n",
    "print(\"F1 Score:\", f1_xgb)\n",
    "print(\"AUC-ROC:\", roc_auc_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avec CV \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Définir les distributions des hyperparamètres à tester\n",
    "param_dist = {\n",
    "    'max_depth': randint(3, 10),  # Limiter la profondeur des arbres\n",
    "    'learning_rate': uniform(0.01, 0.3),  # Distribution continue pour un learning rate flexible\n",
    "    'n_estimators': randint(100, 500),  # Limiter le nombre d'estimations pour éviter le surajustement\n",
    "    'subsample': uniform(0.7, 0.3),  # Subsampling entre 0.7 et 1\n",
    "    'colsample_bytree': uniform(0.7, 0.3),  # Exploration continue pour le colsample_bytree\n",
    "    'alpha': uniform(0, 1),  # Ajouter de la régularisation L1\n",
    "    'lambda': uniform(0, 1)  # Ajouter de la régularisation L2\n",
    "}\n",
    "\n",
    "# Créer le modèle XGBoost \n",
    "xgb_model = XGBClassifier(random_state=42)  \n",
    "\n",
    "# Appliquer RandomizedSearchCV avec validation croisée \n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dist, n_iter=100, cv=4, scoring='accuracy', n_jobs=-1, verbose=1, random_state=42)\n",
    "\n",
    "# Entraîner le modèle avec les meilleurs hyperparamètres trouvés par la recherche aléatoire (le code a mis 2 minutes à s'exécuter sur mon ordinateur)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Afficher les meilleurs paramètres trouvés\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "\n",
    "# Prédire les valeurs pour l'ensemble de test avec le meilleur modèle\n",
    "y_pred_xgb = random_search.best_estimator_.predict(X_test)\n",
    "y_pred_proba_xgb = random_search.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb)\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb)\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb)\n",
    "roc_auc_xgb = roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"XGBoost Model Performance with RandomizedSearchCV:\")\n",
    "print(\"Accuracy:\", accuracy_xgb)\n",
    "print(\"Precision:\", precision_xgb)\n",
    "print(\"Recall:\", recall_xgb)\n",
    "print(\"F1 Score:\", f1_xgb)\n",
    "print(\"AUC-ROC:\", roc_auc_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L'optimisation des hyperparamètres a permis d'améliorer légérement l'ensemble des performances** (par exemple la capacité discriminante avec l'AUC-ROC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappel des indicateurs de performance de la régression logistique : \n",
    "Accuracy: 0.8468581687612208\n",
    "Precision: 0.8492829967808019\n",
    "Recall: 0.895679012345679\n",
    "F1 Score: 0.871864203094487\n",
    "AUC-ROC: 0.9227828114237271\n",
    "\n",
    "Bien que XGBoost soit un modèle plus complexe et généralement plus puissant, dans le cas présent, les **deux méthodes sont au coude-à-coude en termes de performance**. Cela pourrait s’expliquer par le fait que nous traitons un problème de classification binaire, qui s’aligne parfaitement avec l’objectif premier d’une régression logistique. **Cette dernière est le meilleur choix ici en raison de son interprétabilité, largement supérieure à celle du gradient boosting**. Elle permet une analyse plus transparente des effets des facteurs sur la variable cible, un critère d’autant plus crucial dans le cadre de la dépression, que nous cherchons à prédire mais aussi à expliquer. Nous interpréterons tout de même le modèle XGBoost afin de respecter les consignes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interprétations des résultats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interprétation locale et globale du modèle XGBoost par défaut (conformément aux consignes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation globale** : `Feature importance` (mesure l'effet d'une feature sur la fonction de perte) et `SHapley Additive exPlanations (SHAP) Global` (mesure l'impact moyen d'une caractéristique sur les prédictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Récupérer le booster du meilleur modèle\n",
    "booster = random_search.best_estimator_.get_booster()\n",
    "\n",
    "# Obtenir les scores d'importance des caractéristiques\n",
    "importance = booster.get_score(importance_type='gain')  # 'weight', 'gain' ou 'cover'\n",
    "\n",
    "# Convertir en DataFrame\n",
    "importance_df = pd.DataFrame(importance.items(), columns=['Feature', 'Importance'])\n",
    "\n",
    "# Trier les caractéristiques par ordre décroissant d'importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Sélectionner les 15 principales caractéristiques\n",
    "top_features = importance_df.head(15)\n",
    "\n",
    "# Afficher le tableau\n",
    "print(top_features)\n",
    "\n",
    "# Visualiser avec un graphique amélioré\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(top_features['Feature'], top_features['Importance'], color='teal')\n",
    "plt.xlabel('Importance (Gain)')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Top 15 Feature Importance using XGBoost')\n",
    "plt.gca().invert_yaxis()  # Mettre la plus importante en haut\n",
    "plt.xticks(rotation=45)  # Rotation pour améliorer la lisibilité\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats montrent que **certaines caractéristiques ont un impact significatif sur la prédiction du modèle**, notamment la variable `Suicidal_Thoughts` qui apparaît comme la plus influente, avec une importance de 260,25. Cela indique clairement que **la pensée suicidaire joue un rôle central dans la prédiction de la dépression chez les étudiants**, comme on pouvait s'y attendre. Ensuite, la `Academic_Pressure` (98,42) et le `Financial_Stress` (57,06) se révèlent également cruciaux, bien que leur impact soit inférieur à celui des pensées suicidaires. Cela suggère que **la pression académique et les difficultés financières contribuent de manière significative à la détérioration de la santé mentale**. En revanche, des variables comme `Dietary_Habits_Unhealthy` (26,57) et `Age` (15,04), bien que pertinentes, jouent un rôle moins déterminant. D'autres facteurs tels que `Degree_M.Com` (5,02) et `City_Size_Medium` (4,55) ont une importance relativement faible, ce qui suggère qu'ils ont un effet secondaire ou moins direct sur les prédictions. En résumé, **les variables liées à la santé mentale, à la pression académique et au stress sont les plus déterminantes**, ce qui confirme leur rôle primordial dans le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Global\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Créer un explainer SHAP pour XGBoost\n",
    "explainer = shap.Explainer(random_search.best_estimator_)\n",
    "\n",
    "# Calculer les valeurs SHAP pour l'ensemble des données d'entraînement\n",
    "shap_values = explainer(X_train)\n",
    "\n",
    "# Afficher un beeswarm plot\n",
    "shap.initjs()  # Initialisation de JavaScript pour le rendu du plot\n",
    "shap.plots.beeswarm(shap_values)\n",
    "plt.show()\n",
    "\n",
    "# Calculer la moyenne des valeurs SHAP pour chaque variable\n",
    "mean_shap_values = shap_values.values.mean(axis=0)\n",
    "\n",
    "# Créer un DataFrame pour stocker les résultats\n",
    "shap_summary = pd.DataFrame({\n",
    "    'Feature': X_train.columns,  # Nom de la caractéristique\n",
    "    'Mean SHAP': mean_shap_values  # Moyenne des valeurs SHAP (avec signe)\n",
    "})\n",
    "\n",
    "# Trier par ordre décroissant de l'importance SHAP (en valeur absolue)\n",
    "shap_summary = shap_summary.sort_values(by='Mean SHAP', ascending=False)\n",
    "print(shap_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les `valeurs SHAP` permettent de comprendre l'impact de chaque caractéristique sur les prédictions du modèle. Les résultats obtenus pour `Academic_Pressure`, `Suicidal_Thoughts` et `Financial_Stress` indiquent qu'**une pression académique, des pensées suicidaires et du stress financier augmentent la probabilité de dépression (et inversement si les facteurs sont à un niveau bas**). Ces facteurs sont donc des indicateurs clés de l'état mental d'un étudiant. `Dietary_Habits_Unhealthy` a également un effet positif, mais moins marqué, suggérant que de mauvaises habitudes alimentaires peuvent aussi être liées à une détresse mentale. **L'influence de `Age` sur la prédiction est en quelque sorte \"bipolaire\"** : plus l'âge est élevé, plus l'impact sur la prédiction devient négatif, c'est-à-dire qu'il tend à réduire le risque que l'étudiant soit dépressif, tandis que des âges plus jeunes sont associés à des détresses mentales. **Ces résultats confirment ceux obtenus avec la `feature importance`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation locale** : `Individual Conditional Expectation` ou ICE (comment la prédiction d'une instance est impactée si on fait varier la valeur d'une feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICE\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Après l'entraînement du modèle avec RandomizedSearchCV\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Sélectionner une variable \n",
    "features_to_plot = ['Dietary_Habits_Unhealthy'] # exemple\n",
    "\n",
    "# Tracer les courbes ICE pour la caractéristique\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "display = PartialDependenceDisplay.from_estimator(\n",
    "    best_model, X_train, features=features_to_plot, kind=\"individual\", grid_resolution=50, ax=ax\n",
    ")\n",
    "\n",
    "# Ajouter un titre et ajuster les labels\n",
    "plt.title('ICE (Individual Conditional Expectation) for Dietary_Habits_Unhealthy')\n",
    "plt.xlabel('Dietary_Habits_Unhealthy')\n",
    "plt.ylabel('Probability of depression (model prediction)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la plupart des cas, les **`courbes ICE` ont une trajectoire similaire**, ce qui indique que **l'effet observé est relativement homogène parmi les individus (des habitudes alimentaires malsaines tendent à augmenter la probabilité de dépression chez l'ensemble des étudiants**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interprétation des résultats du modèle de régression logistique (meilleur modèle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résumé du modèle de régression logistique\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicateurs de performance du modèle \n",
    "\n",
    "# Matrice de confusion\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "# Précision\n",
    "print(\"Accuracy:\", round(accuracy,4))\n",
    "# Taux de sensibilité\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", round(sensitivity, 4))\n",
    "# Taux de spécificité \n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", round(specificity, 4))\n",
    "# Score F1\n",
    "print(\"F1 Score:\", round(f1, 4))\n",
    "# Pseudo R2\n",
    "print(\"Pseudo R2:\", round(1 - (result.llf / result.llnull),4)) \n",
    "# AUC-ROC\n",
    "print(\"AUC-ROC:\", round(roc_auc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle de `régression logistique` permet d'**analyser les facteurs influençant la probabilité de souffrir de dépression**. Le `pseudo R²` de 0.4857 indique que **près de 49 % de la variabilité de la dépression est expliquée par le modèle**, ce qui est relativement élevé pour cette méthode. Parmi les variables explicatives, des facteurs comme la pression académique (`Academic_Pressure`), les pensées suicidaires (`Suicidal_Thoughts`), le stress financier (`Financial_Stress`) et certains habitudes alimentaires (`Dietary_Habits_Unhealthy`) ont des **effets significatifs**, avec des coefficients positifs, indiquant qu'ils augmentent la probabilité de dépression. En revanche, des variables comme l'âge (`Age`) montrent des effets négatifs.\n",
    "\n",
    "Les résultats des indicateurs de performance montrent que le **modèle a classifié correctement 84.69 % des cas** (`précision`). Nous pouvons également lire que le modèle détecte correctement la dépression dans 89.57 % des cas où elle est présente (`sensibilité`). Par contre, il distingue un peu moins bien les individus non dépressifs (`spécificité` est de 77.9 %). Le `score F1` de 0.8719 reflète un **équilibre relativement bon entre la précision et la sensibilité**. Enfin, l'`AUC-ROC` de 0.9228 montre une très bonne capacité de discrimination entre les individus souffrant de dépression et ceux ne souffrant pas. Les effets marginaux, calculés ci-après, permettront une interprétation plus précise des relations entre les variables et la probabilité de dépression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effets des variables explicatives\n",
    "\n",
    "# Calcul des effets marginaux\n",
    "marginal_effects = result.get_margeff()\n",
    "\n",
    "# Résumé des effets marginaux avec p-values\n",
    "print(marginal_effects.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les `effets marginaux` sont une méthode d'analyse utilisée pour interpréter les résultats d'un modèle de régression logistique en fournissant l'**impact d'une variation d'une unité d'une variable explicative sur la probabilité d'un événement (ici, la `dépression`)**. Voici l'interprétation des principaux résultats obtenus :\n",
    "\n",
    "**Des `pensées suicidaires` (0.2757, p < 0.0001), des `habitudes alimentaires malsaines` (0.1154, p < 0.0001) et la `pression académique` (0.0917, p < 0.0001) augmentent fortement la probabilité de `dépression`** (effet marginal positif important et significatif). Concrètement cela signifie par exemple qu'une augmentation d'une unité sur l'échelle de la pression scolaire ressentie (par rapport à la valeur moyenne de 3 environ) augmente de 0.09 la probabilité que l'étudiant souffre de dépression. \n",
    "\n",
    "On recense **peu de variables tendant à réduire la probabilité de `dépression` chez les étudiants**. Dans ce cas de figure on trouve notamment la `satisfaction dans les études` (-0.0257, p < 0.0001).\n",
    "En conclusion, **certains facteurs jouent un rôle prépondérant pour expliquer la `dépression` chez les étudiants indiens interrogés**, comme les pensées suicidaires, les habitudes alimentaires, la pression académique. Le **sens des relations significatives identifiées est presque toujours cohérent**. Plusieurs variables semblent être peu pertinentes pour expliquer le phénomène étudié, telles que la taille de la ville et le sexe. **Ces résultats confirment ceux obtenus avec la méthode `XGBoost`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce projet, nous avons cherché à **comprendre et prédire la présence de dépression chez les étudiants en Inde** en exploitant un dataset de **27 901 observations** intégrant des informations **démographiques, académiques et comportementales**. Pour identifier les **facteurs les plus influents**, nous avons mobilisé **deux approches complémentaires** : **XGBoost**, reconnu pour sa capacité à capturer des relations complexes, et **la régression logistique**, privilégiée pour son interprétabilité et son adéquation aux problèmes de classification binaire.  \n",
    "\n",
    "Après un **prétraitement minutieux des données** (gestion des valeurs atypiques, transtypage…) et une **analyse statistique approfondie**, nous avons entraîné et ajusté les modèles en veillant notamment au **traitement de la multicolinéarité**.  \n",
    "\n",
    "L’évaluation des performances a montré que **XGBoost offrait une meilleure capacité prédictive**. Cependant, l’analyse des relations entre les **facteurs influents** et l’état de dépression – notamment via le **beeswarm plot** – a révélé une forte linéarité des effets. Dans ce contexte, **la régression logistique s’impose comme l’outil le plus pertinent**, car elle permet d’identifier des relations similaires tout en offrant une **interprétation claire et exploitable**.  \n",
    "\n",
    "Dans un domaine aussi sensible que **la santé mentale**, la **transparence** des modèles est primordiale pour garantir une **compréhension accessible** par les professionnels et faciliter l’**élaboration de politiques publiques adaptées**. Contrairement à XGBoost, qui agit comme une **\"boîte noire\"**, la régression logistique permet de **quantifier directement l’impact des facteurs**, renforçant ainsi la **fiabilité et l’applicabilité des conclusions**.  \n",
    "\n",
    "Nos résultats mettent en avant **quatre facteurs déterminants** dans la prédiction de la dépression : **la pression académique**, reflet des attentes élevées pesant sur les étudiants ; **les pensées suicidaires**, indicateur direct d’une détresse psychologique sévère ; **le stress financier**, qui accentue l’anxiété et le mal-être ; et enfin **une alimentation déséquilibrée**, soulignant un lien potentiel entre l’hygiène de vie et la santé mentale.  \n",
    "\n",
    "Ces résultats confirment les tendances mises en avant dans la littérature et rappellent l’importance de prendre en compte ces facteurs dans les **stratégies de prévention et d’accompagnement des étudiants en difficulté**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DISCUSSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une limite importante à la généralisation de nos résultats est que l'ensemble de données se concentre sur **une seule année** (par ailleurs non renseignée). Cela restreint l’interprétation des résultats à une **photographie statique** de la situation des étudiants et empêche toute **analyse longitudinale**. En conséquence, il est impossible d’examiner la dynamique des facteurs influençant la dépression ou d’évaluer l’impact de chocs exogènes comme la pandémie de COVID-19. \n",
    "\n",
    "De plus, nous pourrions chercher à **accroître sa portée en étendant l'analyse à d’autres pays** afin d’évaluer la robustesse des résultats dans des contextes variés. Il serait également pertinent d'**intégrer des variables concernant l'entourage de l'étudiant**, comme le soutien social, qui peut jouer un rôle important dans la santé mentale.\n",
    "\n",
    "Les limites soulevées sont finalement des **opportunités d'amélioration**, permettant d’accroître la compréhension des facteurs de risque pour la santé mentale des étudiants et d'affiner les politiques publiques mises en place."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
